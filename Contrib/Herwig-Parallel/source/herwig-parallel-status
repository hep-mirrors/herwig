#! /usr/bin/env python

## --------------------
## Herwig-Parallel
## --------------------
## Author: Daniel Rauch
## Date:   25 Feb 2015
## --------------------

import sys
import os
import re
import signal
from subprocess import call
from subprocess import check_output
from math import sqrt
import datetime
from ConfigParser import SafeConfigParser
from optparse import OptionParser
from hwp import checkConfig


### ------------
### catch Ctrl+C
### ------------
signal.signal(signal.SIGINT, lambda x,y: exit())


### ------------------------------
### calculate median value of list
### ------------------------------
def median(mylist):
  copiedlist = [x for x in mylist]
  while 0.0 in copiedlist: copiedlist.remove(0.0)
  sorted_list = sorted(copiedlist)
  l = len(sorted_list)
  if not l%2 == 0:
    return(sorted_list[(l-1)/2])
  else:
    return((sorted_list[(l+1)/2]+sorted_list[(l-1)/2])/2.0)


### ------------
### main program
### ------------
def main():

  # load Herwig-Parallel configuration file
  configFileName = os.path.dirname(os.path.realpath(sys.argv[0])) + "/../config/herwig-parallel.conf"
  configParser = SafeConfigParser()
  configParser.read(configFileName)

  # load cluster configuration file
  configFileNameClusters = os.path.dirname(os.path.realpath(sys.argv[0])) + "/../config/clusters.conf"
  configParserClusters = SafeConfigParser()
  configParserClusters.read(configFileNameClusters)

  # load queue configuration file
  configFileNameQueues = os.path.dirname(os.path.realpath(sys.argv[0])) + "/../config/queues.conf"
  configParserQueues = SafeConfigParser()
  configParserQueues.read(configFileNameQueues)

  # sanity checks for cluster and queue configuration
  checkConfig(configParserClusters,configParserQueues)

  # setup command line options & parameters
  optionParser = OptionParser(usage="%prog runname")
  optionParser.add_option("-c", "--no-consistency-checks", action="store_false", dest="consistency_checks", default=True, help="do not display consistency checks [default: display consistency checks]")
  optionParser.add_option("-l", "--no-job-list", action="store_false", dest="job_list", default=True, help="do not display status of individual jobs [default: display list with job statuses]")
  optionParser.add_option("-o", "--no-file-output", action="store_false", dest="file_output", default=True, help="do not output status information into file [default: output status information into file]")
  optionParser.add_option("-r", "--no-run-info", action="store_false", dest="run_info", default=True, help="do not display run information [default: display run information]")
  optionParser.add_option("-s", "--no-summary", action="store_false", dest="summary", default=True, help="no not display summary [default: display summary]")
  opts, args = optionParser.parse_args()

  # sanity checks for command line arguments
  if len(args) <> 1:
    sys.stderr.write("Must specify an unambiguous name of the parallel run whose status is to be given!\n")
    sys.exit(1)

  run_name = args[0].strip('/')
  cwd = os.getcwd()

  # check if run exists
  if not os.path.exists(run_name):
    sys.stderr.write("The specified run "+run_name+" does not exist!\n")
    sys.exit(1)

  normalLetters = '\x1b[0m'
  redLetters = '\x1b[31m'
  greenLetters = '\x1b[32m'
  yellowLetters = '\x1b[33;1m'
  darkblueLetters = '\x1b[34m'
  magentaLetters = '\x1b[35m'
  cyanLetters = '\x1b[36m'

  line = '==============================='
  for i in range(len(run_name)): line += '='
  print(line)
  print("HERWIG-PARALLEL: Status of run {}".format(run_name))
  print(line)
  print("")

  # read required information from 'run.info' file
  integrationjobids = []
  jobs = []
  job_ids = []
  total_events = []
  queues = []
  f_run_info = open(run_name+'/run.info','r')
  l = 0
  for line in f_run_info:
    l += 1
    if l == 1: cluster = line.replace('\n','')
    elif l == 2:
      infile = line.replace('\n','')
      infile_title = infile.split('/')[-1]
    elif l == 3:
      setupfile = line.replace('\n','')
      setupfile_title = setupfile.split('/')[-1]
    elif l == 4:
      generator = line.replace('\n','')
      runfile = generator+'.run'
    elif l == 5:
      buildscript = line.replace('\n','')
      buildscript_title = buildscript.split('/')[-1]
    elif l == 7:
      integratescript = line.replace('\n','')
      integratescript_title = integratescript.split('/')[-1]
    elif l == 8: integrationjobids = line.replace('\n','').split()
    elif l == 9:
      runscript = line.replace('\n','')
      runscript_title = runscript.split('/')[-1]
    elif l > 11:
      data = line.replace('\n','').split()
      jobs.append(int(data[1].strip('#').strip(':')))
      job_ids.append(data[3])
      total_events.append(int(data[7]))
  f_run_info.close()

  finishedReadStep = os.path.exists(run_name+'/read/'+runfile)
  activeRunStep = len(job_ids) > 0

  try:
    call(configParserClusters.get(cluster,'joblist')+' > '+run_name+'/tmp', shell=True)
  except:
    sys.exit(0)
  ansi_escape = re.compile(r'\x1b[^m]*m')          # remove ANSI color escape characters

  runningIntegrationJobs = 0
  queuedIntegrationJobs = 0
  for j in integrationjobids:
    status = ansi_escape.sub('', check_output('grep '+j+' '+run_name+'/tmp | ' + configParserClusters.get(cluster,'status'), shell=True))
    if status == configParserClusters.get(cluster,'statusQueued'):
      queuedIntegrationJobs += 1
    elif status == configParserClusters.get(cluster,'statusRunning'):
      runningIntegrationJobs += 1
  finishedIntegrationJobs = len(integrationjobids) - runningIntegrationJobs - queuedIntegrationJobs

  if opts.run_info:
    print("---------------")
    print("run information")
    print("---------------")
    print("read step:    {} ({}/{} integration jobs finished, {} queued, {} running)".format("complete" if finishedReadStep else "not yet finished", finishedIntegrationJobs, len(integrationjobids), queuedIntegrationJobs, runningIntegrationJobs))
    # print("run step:     {}".format("underway" if activeRunStep else "not yet started"))
    print("")
    print("location:          {}/{}".format(cwd, run_name))
    print("infile:            {}".format(infile))
    print("setupfile:         {}".format(setupfile))
    print("build script:      {}".format(buildscript))
    print("integrate script:  {}".format(integratescript if l > 6 else ''))
    print("run script:        {}".format(runscript if l > 8 else ''))
    print("")

  if not finishedReadStep or not activeRunStep:
    sys.exit(0)

  if opts.job_list:
    print("-------------------------")
    print("status of individual jobs")
    print("-------------------------")
  else:
    print("collecting job statuses...\n")

  i = 0
  progress = []
  current_events = []
  current_attempts = []
  cs = []
  err = []
  err_att = []
  iUndone = 0
  iQueue = 0
  iRead = 0
  iRun = 0
  iFinished = 0
  iCrashedRead = 0
  iCrashedRun = 0
  iUnknown = 0
  jobsUndone = []
  jobsQueued = []
  jobsRead = []
  jobsRun = []
  jobsFinished = []
  jobsCrashedRead = []
  jobsCrashedRun = []
  jobsUnknown = []

  for j in range(len(jobs)):
    
    fileParallel = run_name+'/'+str(jobs[j])+'/'+generator+('' if setupfile=='' else '-'+setupfile_title)+'.parallel'
    
    # get job status: queued / running / not running
    job_queued = False
    job_running = False
    status = ansi_escape.sub('', check_output('grep '+job_ids[j]+' '+run_name+'/tmp | ' + configParserClusters.get(cluster,'status'), shell=True))
    if status == configParserClusters.get(cluster,'statusQueued'):
      job_queued = True
      host = 'queued'
    elif status == configParserClusters.get(cluster,'statusRunning'):
      if os.path.isfile(fileParallel):
        job_running = True
        host = open(fileParallel,'r').readline().rstrip().replace('hostname> ','')
      else:
        job_queued = True
        host = 'queued'
    else:
      host = 'not queued/running'
    
    if not os.path.isfile(fileParallel): # job has not been started so far
      progress.append(0.0)
      current_events.append(0)
      current_attempts.append(0)
      cs.append(0.0)
      err.append(0.0)
      err_att.append(0.0)
      if job_queued:
        if opts.job_list: print("job #{} with id {} ({}): job queued but not yet started".format(jobs[j], job_ids[j], host))
        iQueue += 1
        jobsQueued.append(jobs[j])
      else:
        if opts.job_list: print("job #{} with id {} ({}): job has not been processed yet (not sent to cluster)".format(jobs[j], job_ids[j], host))
        iUndone += 1
        jobsUndone.append(jobs[j])
      continue
    elif os.path.isfile(fileParallel):
      output = check_output("grep 'event>' "+fileParallel+' | tail -n 1', shell=True)
      output_split = output.split()
      if len(output_split) > 1:
        tmp = output_split[1].split('/')
        progress.append(float(tmp[0]) / float(tmp[2]))
        current_events.append(int(tmp[0]))
        current_attempts.append(int(tmp[1]))
        cs.append(float(output_split[4]))
        err.append(float(output_split[7]))
        err_att.append(err[j]*sqrt(float(current_attempts[j])))
        if opts.job_list: print("job #{} with id {} ({}): {:5.1f}% done, xs = {: f} pb +/- {:f} pb ({:f})".format(jobs[j], job_ids[j], host, progress[j]*100.0, cs[j], err[j], err_att[j]))
        if job_running:
          iRun += 1
          jobsRun.append(jobs[j])
        else:
          if progress[j] == 1.0:
            iFinished += 1
            jobsFinished.append(jobs[j])
          else:
            iCrashedRun += 1
            jobsCrashedRun.append(jobs[j])
      else:
        progress.append(0.0)
        current_events.append(int(0))
        current_attempts.append(int(0))
        cs.append(0.0)
        err.append(0.0)
        err_att.append(0.0)
        if opts.job_list: print("job #{} with id {} ({}): no events generated yet/unknown status".format(jobs[j], job_ids[j], host))
        iUnknown += 1
        jobsUnknown.append(jobs[j])
    else: # len(output_split) == 0: # job was started, but no events generated yet (still reading or may have crashed in read step)
      progress.append(0.0)
      current_events.append(0)
      current_attempts.append(0)
      cs.append(0.0)
      err.append(0.0)
      err_att.append(0.0)
      if opts.job_list: print("job #{} with id {} ({}): job was started, but no events generated yet".format(jobs[j], job_ids[j], host))
      if job_running:
        iRead += 1
        jobsRead.append(jobs[j])
      else:
        iCrashedRead += 1
        jobsCrashedRead.append(jobs[j])
      continue
  os.remove(run_name+'/tmp')
  if opts.job_list: print("")

  n = 0
  TOTAL_EVENTS = 0
  CURRENT_EVENTS = 0
  CURRENT_ATTEMPTS = 0
  CS = 0.0; ERR = 0.0
  MED_CS = 0.0
  MED_ERR_ATT = 0.0
  for j in range(len(jobs)):
    TOTAL_EVENTS += total_events[j]
    CURRENT_EVENTS += current_events[j]
    CURRENT_ATTEMPTS += current_attempts[j]
    if progress[j] > 0.0:
      n += 1
      CS += current_attempts[j]*cs[j]
      ERR += current_attempts[j]*cs[j]**2.0+current_attempts[j]*(current_attempts[j]-1)*err[j]**2.0
  PROGRESS = float(CURRENT_EVENTS) / float(TOTAL_EVENTS)
  if CURRENT_ATTEMPTS > 1:
    ERR = sqrt(ERR - CS**2.0/CURRENT_ATTEMPTS)/sqrt(CURRENT_ATTEMPTS*(CURRENT_ATTEMPTS-1))
    CS /= CURRENT_ATTEMPTS
  else:
    CS = 0.0
    ERR = 0.0
  if n > 0:
    MED_CS = median(cs)
    MED_ERR_ATT = median(err_att)

  # check if jobs are consistent or not
  devs = float(configParser.get('consistency-checks','standard-deviations'))
  error_factor = float(configParser.get('consistency-checks','error-factor'))
  inconsistent_jobs_cs = []
  inconsistent_jobs_err = []
  for j in range(len(jobs)):
    if progress[j] > 0.0:
      myerr = MED_ERR_ATT/sqrt(current_attempts[j])
      if cs[j] < MED_CS-devs*myerr or cs[j] > MED_CS+devs*myerr:
        inconsistent_jobs_cs.append(j)
      if err_att[j] < MED_ERR_ATT/error_factor or err_att[j] > MED_ERR_ATT*error_factor:
        inconsistent_jobs_err.append(j)
  inconsistent_jobs = list(set(inconsistent_jobs_cs + inconsistent_jobs_err))

  if opts.consistency_checks:
    print("------------------")
    print("consistency checks")
    print("------------------")
    #print("")
    print("median cross section:        {:f} pb".format(MED_CS))
    print("median integrand variance:   {:f}".format(MED_ERR_ATT))
    print("")
    print("criteria for consistency limits:")
    print("- standard deviations wrt. to median cross section:   {}".format(devs))
    print("- error factor wrt. median integrand variance:        {}".format(error_factor))
    print("")
    print(("=> {} inconsistent job" + ("s" if len(inconsistent_jobs)<>1 else "") + " found with given criteria" + (":" if len(inconsistent_jobs)>0 else "")).format("no" if len(inconsistent_jobs)==0 else len(inconsistent_jobs)))
    #for i in inconsistent_jobs: print("- job #{} with id {}: {} pb +/- {} pb".format(i+1,job_ids[i],cs[i],err[i]))
    for i in sorted(inconsistent_jobs):
      errstring = "CS" if i in inconsistent_jobs_cs else "  "
      errstring += " " + ("&" if (i in inconsistent_jobs_cs and i in inconsistent_jobs_err) else " ") + " "
      errstring += "ERR" if i in inconsistent_jobs_err else "   "
      print("- job #{} [{}]: {: f} pb +/- {:f} pb ({:f})".format(i+1,errstring,cs[i],err[i],err_att[i]))
    print("")

  if opts.summary:
    cont = ['..']
    numDisplay = 10
    print("-------")
    print("summary")
    print("-------")
    print("total number of jobs:                           {:7d}".format(len(job_ids)))
    print("- unprocessed jobs (not sent to cluster):       {:7d}   {}".format(iUndone, repr(jobsUndone).replace(',','') if len(jobsUndone)>0 else ""))
    print("- queued jobs (not yet started):                {:7d}   {}".format(iQueue, repr(jobsQueued[:numDisplay]+(cont if len(jobsQueued)>numDisplay else [])).replace(',','').replace("'",'') if len(jobsQueued)>0 else ""))
    print("- running jobs (but no events generated yet):   {:7d}   {}".format(iRead, repr(jobsRead[:numDisplay]+(cont if len(jobsRead)>numDisplay else [])).replace(',','').replace("'",'') if len(jobsRead)>0 else ""))
    print("- running jobs (generating events):             {:7d}   {}".format(iRun, repr(jobsRun[:numDisplay]+(cont if len(jobsRun)>numDisplay else [])).replace(',','').replace("'",'') if len(jobsRun)>0 else ""))
    print("- finished jobs:                                {:7d}   {}".format(iFinished, repr(jobsFinished[:numDisplay]+(cont if len(jobsFinished)>numDisplay else [])).replace(',','').replace("'",'') if len(jobsFinished)>0 else ""))
    print("- jobs crashed/aborted in read phase            {:7d}   {}".format(iCrashedRead, repr(jobsCrashedRead).replace(',','') if len(jobsCrashedRead)>0 else ""))
    print("- jobs crashed/aborted in run phase             {:7d}   {}".format(iCrashedRun, repr(jobsCrashedRun).replace(',','') if len(jobsCrashedRun)>0 else ""))
    print("- jobs with unknown status:                     {:7d}   {}".format(iUnknown, repr(jobsUnknown).replace(',','') if len(jobsUnknown)>0 else ""))
    print("")

  print("-----------------------------------")
  print("combination of total cross sections")
  print("-----------------------------------")
  print("current progress: {:5.1f}% ({} of {} events done)".format(PROGRESS*100.0,CURRENT_EVENTS,TOTAL_EVENTS))
  print("current result:   {:f} pb +/- {:f} pb ( = {:f}%)".format(CS, ERR, ERR/CS*100.0 if CS <> 0.0 else 0.0))
  print("")

  if opts.file_output:
    run_status = os.path.isfile(run_name+'/run.status')
    f_result = open(run_name+'/run.status','a')
    if not run_status:
      f_result.write("# date/time   current events   current attempts   total events   cs   err\n")
    f_result.write("{}   {}   {}   {}   {: f}   {: f}\n".format(datetime.datetime.now().isoformat(), CURRENT_EVENTS, CURRENT_ATTEMPTS, TOTAL_EVENTS, CS, ERR))
    f_result.close()


### ------------------------------------------
### remove '^C' outpt when exiting with Ctrl+C
### ------------------------------------------
def exit():
  sys.stderr.write("\r\033[K")
  sys.exit(1)


### -----------------
### call main program
### -----------------
if __name__ == '__main__':
  main()
